## GeneralLM

`GeneralLM` is a code that shows the trend of NLP(Natural Language Processing) after Transformer(Attention is all you need) in a very short, simple code similar to [nlp-tutorial](https://github.com/graykode/nlp-tutorial).



#### List up Paper to be implementated

- Transformer([Attention is all you need](https://arxiv.org/abs/1706.03762), 2017.06)

- Weight Transformer([Weighted Transformer Network for Machine Translation](https://arxiv.org/abs/1711.02132), 2017.11)
- ELMo([Deep contextualized word representations](https://arxiv.org/abs/1802.05365), 2018.02)
- LISA([Linguistically-Informed Self-Attention for Semantic Role Labeling](https://arxiv.org/abs/1804.08199), 2018.04)

- OpenAI GPT([Improving Language Understanding by Generative Pre-Training](<https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf>), 2018.06)
- Universal Transformer([Universal Transformers](https://arxiv.org/abs/1807.03819), 2018.07)

- BERT([BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), 2018.10)
- Transformer XL([Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860), 2019.01)